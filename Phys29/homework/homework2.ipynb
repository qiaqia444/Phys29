{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px; text-align: center;\">Introduction to Computer Programming for the Physical Sciences</h1>\n",
    "<h2 style=\"font-size: 24px; text-align: center;\">Joseph F. Hennawi</h2>\n",
    "<h3 style=\"font-size: 24px; text-align: center;\">Winter 2024</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"list-style: none;\">\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a new Jupyter notebook</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Name your notebook with your name and Homework 1</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a Markdown cell at the top and write your name and Homework 1</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Open a Markdown cell before each problem and write e.g. Problem 1, Problem 2(a), etc.</li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Please abide by the <b><a href=\"https://github.com/enigma-igm/Phys29/blob/main/using_AI_tools.md\">Policy and Guidelines on Using AI Tools</a></b></li>\n",
    "  <li style=\"margin-bottom: 10px; font-size: 20px;\"><span style=\"display: inline-block; width: 10px; height: 10px; border: 2px solid black; margin-right: 10px;\"></span>Once you finish the problems: 1) Restart the Python kernel and clear all cell outputs. 2) Rerun the notebook from start to finish so that all answers/outputs show up. 3) Save your notebook as a single .pdf file and upload it to Gradescope on Canvas by the deadline. <b>No late homeworks will be accepted except for illness accompanied by a doctor's note.</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Problem 1: Range of a Projectile\n",
    "Write a Python function that computes the range of a projectile that is launched from height $y_0$ above the ground at speed $v_0$ at angle $\\alpha$ above the horizontal direction.  By *range*, I mean the horizontal distance traveled before reaching the ground.  Neglect air resistance and Coriolis effects.  For $y_0=11~{\\rm m}$ and $v_0=13~{\\rm m~s^{-1}}$, which of the following angles gives the biggest range: $\\alpha=20^\\circ, 30^\\circ, 40^\\circ, 50^\\circ, 60^\\circ$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Fun with Conditional Statments\n",
    "Using ${\\tt if}, {\\tt elif}, {\\tt else}$ statements, write a Python function that takes any three distinct real input numbers $a$, $b$, and $c$, and returns the same values in a tuple in order of smallest to largest. For example, if $a=3$, $b=1$, and $c=2$, then the function should return the tuple $(1,2,3)$.  If $a=3$, $b=2$, and $c=3$, then the function should return the tuple $(2,3,3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Machine Epsilon\n",
    "In the lecture we discussed the limited precision of floating point numbers and floating point arithmetic. An important value to quantity is floating-point accuracy which is referred to as the *machine epsilon*. Please read this [Wikipedia article on the machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon) to learn more about this important concept. \n",
    "\n",
    "The machine epsilon is defined as the smallest number $\\epsilon_m$ such that $1 + \\epsilon_m > 1$. According to the Wikipedia article, the machine epsilon in python can be estimated to within a factor of two via the algorithm:\n",
    "```python\n",
    "epsilon_m = 1.0\n",
    "while (1.0 + 0.5*epsilon_m) != 1.0:\n",
    "    epsilon_m /= 2.0\n",
    "```\n",
    "\n",
    "**a)** Write a python function that implements this algorithm and returns the machine epsilon. Which float-type is used in Python (see the table of the Wikipedia article)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**b)** In lecture it was argued that in Python the smallest number that can be represented in python is about `1e-308`, which is many orders of magnitude smaller than the \\( \\epsilon_m \\) that you just derived. What is the difference between the smallest representable floating point number and the machine epsilon?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**c)** Consider 32bit floating point numbers, or so called single-precision. To within an order of magnitude estimate the machine epsilon, the smallest number that can be represented, and the largest number that can be represented. Repeat your estimates for 8bit floating point numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Numerical Derivatives\n",
    "In this problem we will explore the accuracy of numerical derivatives. Consider the function $f(x) = x^2(x-1)$\n",
    "\n",
    "**a)** Analytically compute $f^\\prime(x)$ and evauate it at $x=1.0$.\n",
    "\n",
    "**b)** Write a python function that estimates the derivative of $f(x)$ numerically using the forward difference formula:\n",
    "$$\n",
    "f^\\prime(x) \\approx \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "where $h$ is a small number.\n",
    "**c)** Write another python function that estimates the derivative of $f(x)$ numerically using the symmetric difference formula:\n",
    "$$\n",
    "f^\\prime(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
    "$$\n",
    "**d)** Calculate $f^\\prime(1.0)$ using your two numerical derivative functions for $h=10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, ....$ until something *really bad happens* (see below). Print out both $h$ and $f^\\prime(1.0)$, as $h$ becomes smaller and smaller. Format the output of $f^\\prime(1.0)$ to show 16 digits after the decimal point. Do the calculation using the built-in python float data type (nothing fancy please!)\n",
    "\n",
    "**e)** Based on your outputs from above, you should see that the symmetric difference formula is always more precise at a given value of $h$. To understand why this is the case we need a bit of calculcus. The Taylor expansion of $f(x)$ around $x$ is given by:\n",
    "$$\n",
    "f(x+h) = f(x) + h f^\\prime(x) + \\frac{h^2}{2} f^{\\prime\\prime}(x) + \\frac{h^3}{3!} f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!} f^{\\prime\\prime\\prime\\prime}(x) + \\cdots\n",
    "$$\n",
    "which states that for small values of $h$, the function can be expanded as a sum of powers of $h$ and higher order derivatives of $f(x)$. \n",
    "\n",
    "Derive expressions for $f^\\prime(x)$ using the Taylor expansion above for the two numerical derivative formulas that we employed in part (b) and (c).\n",
    "\n",
    "**f)** Based on your answers to part (e), explain why the symmetric difference formula is always more precise than the forward difference formula in the limit $h\\rightarrow 0$. Note that the amount of computational work for both of the derivative estimators is the same, i.e. the function is evaluated at two locations, and then division by $h$ or $2h$ is performed. Hence, this problem illustrates that numerical derivatives should always be calculated  using symmetric differences whenever possible. \n",
    "\n",
    "**g)** As $h$ becomes *too small* the precision of both of the derivative estimators starts to degrade. This is because when $h$ is extremely small, taking the difference of $f(x+h)$ and $f(x - h)$ (or $f(x+h)$ and $f(x)$) becomes problematic as you are subtracting two numbers that are very close to each other. Read this Wikipedia article on [catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation) and describe in your own words why the numerical preision degrades when $h$ becomes too small.\n",
    "\n",
    "It can be shown that catastrophic cancellation starts to degrade results when $h\\approx x\\sqrt{\\epsilon_m}$ (forward difference estimator) or \n",
    "$h\\approx x\\epsilon_m^{2/3}$ (symmetric difference estimator), where $\\epsilon_m\\simeq 2\\times 10^{-16}$ is the machine epsilon that we derived in Problem 3. These formulas are reliable provided that $x$ is not too close to zero (in our problem $x=1$ so we are okay). For more background on where these scalings come from see Chapter 5.7 of [Numerical Recipes](http://tinyurl.com/yc36ac7z)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
